# -*- coding: utf-8 -*-
"""Nickson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wyvilibd4254XrpIeYeDTomuoy-WatZN
"""

#make sure you have installed the libraries before importing them
import numpy as np
import pandas as pd
import scipy.stats as st
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency
from statsmodels.formula.api import ols
from statsmodels.stats.anova import anova_lm
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score
from sklearn import metrics
from sklearn.metrics import roc_curve, auc, roc_auc_score

df = pd.read_csv('Student Mental health.csv')
df.head()

df.columns = ['Date_Time', 'Gender', 'Age', 'Course', 'Year', 'CGPA', 'Marital_Status', 'Depression', 'Anxiety', 'Panic_Attack', 'Treatment']
df.head()

df.isnull().sum()

df['Year'].unique().tolist()

def cleanText(text):
    text = text[-1]
    text = int(text)
    return text
df["Year"] = df["Year"].apply(cleanText)
print("First 5 value after Cleaning the text of Year Column")
print(df["Year"][:5], "\n")

df.Age.fillna(21, inplace=True)
df.Age.isnull().sum()

df['CGPA'].unique().tolist()

def remove_space(delimstrng):
    delimstrng = delimstrng.strip()
    return delimstrng
df["CGPA"] = df["CGPA"].apply(remove_space)
print("First five values of CGPA after cleaning the space from CGPA column:")
print(df["CGPA"][:5], "\n")
print(df['CGPA'].unique().tolist())

course_list = df['Course'].unique().tolist()
print(course_list,'\n','\n','Number Of courses -',len(course_list))

#Let's replace redundant course name with the standard course name
df['Course'].replace({'engin': 'Engineering' , 'Engine':'Engineering' , 'Islamic education':'Islamic Education' , 'Pendidikan islam':'Pendidikan Islam' , 'BIT':'IT', 'psychology':'Psychology', 'koe': 'Koe', 'Kirkhs': 'Irkhs', 'KIRKHS': 'Irkhs', 'Benl': 'BENL', 'Fiqh fatwa ': 'Fiqh', 'Laws': 'Law'} , inplace = True)

course_list = df['Course'].unique().tolist()
print(course_list,'\n\n','Number Of courses -',len(course_list))

course_per_student = df.Course.value_counts()
course_per_student

df.drop('Date_Time', axis=1, inplace=True)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
categorical_columns= [x for x in df.columns if df.dtypes[x] == 'object']
for column in categorical_columns:
    df[column] = encoder.fit_transform(df[column])
df.head()

#correlation matrix
corrmat= df.corr()
plt.figure(figsize=(10,10))
sns.heatmap(corrmat,annot=True, cmap=None)

import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.datasets import fetch_openml
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import KBinsDiscretizer, MinMaxScaler, OneHotEncoder
import pandas as pd
from sklearn.utils import all_estimators

X = df.drop(["Depression"],axis=1)
y = df["Depression"]

#spliting test and training sets
X_train, X_test, y_train,y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

imputer = make_column_transformer(
    (SimpleImputer(strategy="median"), [1,3]),
    (SimpleImputer(strategy="most_frequent"), [0,2,4,5,6,7,8])
)
normalizer = make_column_transformer(
    (MinMaxScaler(feature_range=(0, 1)), [0,1]),
    remainder="passthrough"
)

encoder = make_column_transformer(
    (OneHotEncoder(handle_unknown="ignore", sparse=False), [2,3,4,5,6,7,8]),
    remainder="passthrough"
)

preprocessing = make_pipeline(imputer, normalizer, encoder)

clfL = Pipeline(
    steps=[("preprocessor", preprocessing), ("classifier", LogisticRegression(random_state=42))]
)
clfD = Pipeline(
    steps=[("preprocessor", preprocessing), ("classifier", DecisionTreeClassifier(criterion="entropy",random_state=42))]
)
clfD2 = Pipeline(
    steps=[("preprocessor", preprocessing), ("classifier", DecisionTreeClassifier(random_state=42))]
)
clfR = Pipeline(
    steps=[("preprocessor", preprocessing), ("classifier", RandomForestClassifier())]
)
clfS = Pipeline(
    steps=[("preprocessor", preprocessing), ("classifier", SVC())]
)

# List of all the pipelines
pipelines = [clfL, clfD, clfD2, clfR,clfS]

# Dictionary of pipelines and classifier types for ease of reference
pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree entropy',2:'Decision Tree gini' ,3: 'RandomForest', 4: "SVC"}


# Fit the pipelines
for pipe in pipelines:
    pipe.fit(X_train, y_train)

#cross validation on accuracy
cv_results_accuracy = []
for i, model in enumerate(pipelines):
    cv_score = cross_val_score(model, X_train,y_train, cv=10 )
    cv_results_accuracy.append(cv_score)
    print("%s: %f " % (pipe_dict[i], cv_score.mean()))